# ğŸ³ Sistema RAG para RecomendaciÃ³n y ExplicaciÃ³n de Recetas
### RecuperaciÃ³n semÃ¡ntica y generaciÃ³n de instrucciones de cocina en espaÃ±ol

---

## ğŸ‘¤ Autores
- **Gabriela Zamora**
- **Daniel**
- **Felipe Rosas**
- **SofÃ­a Angulo**

---

## ğŸ¯ DescripciÃ³n del Proyecto

Este proyecto implementa un **asistente inteligente de cocina** basado en **RAG (Retrieval-Augmented Generation)**.  
El sistema permite que un usuario escriba una consulta en espaÃ±ol, por ejemplo:

> â€œTengo arroz y huevo, quiero algo rÃ¡pido para el desayuno y sin horno.â€

El asistente combina dos componentes:

- ğŸ” **RecuperaciÃ³n semÃ¡ntica:** encuentra recetas relevantes usando un encoder multilingÃ¼e.
- âœ¨ **GeneraciÃ³n de texto:** explica la receta paso a paso usando un modelo generativo open-source.

Este proyecto aplica conceptos como **Transformers, embeddings, bÃºsqueda semÃ¡ntica, retrieval, decoders y arquitectura RAG**.

---

## ğŸ“š Dataset

### CaracterÃ­sticas principales
- **Fuente:** Hugging Face  
- **Dataset:** `m3hrdadfi/recipe_nlg_lite`  
- **TamaÃ±o:** 7,000+ recetas  
- **Columnas clave:**
  - name  
  - ingredients  
  - steps  
  - description  

Para el prototipo se usa un subconjunto de 300 recetas.

---

## ğŸ§  Arquitectura del Sistema

### **1. Encoder (Retriever)**
- Modelo: `paraphrase-multilingual-MiniLM-L12-v2`
- Tipo: Transformer **encoder-only**
- FunciÃ³n: generar **embeddings semÃ¡nticos** para comparar consultas y recetas.

### **2. Vector Store**
Almacenado temporalmente en memoria (NumPy).  
Contiene:
- embeddings de recetas  
- metadatos (nombre, ingredientes, pasos)

Permite recuperaciÃ³n por **similitud de coseno**.

### **3. Decoder (Generative Model)**
- Modelo: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`
- Tipo: Transformer **decoder-only**
- FunciÃ³n:
  - elegir la receta mÃ¡s adecuada  
  - explicar pasos numerados  
  - usar lenguaje simple  
  - no inventar ingredientes  

---

## ğŸš€ Flujo de Trabajo

1. Carga y preprocesamiento del dataset  
2. GeneraciÃ³n de embeddings con el encoder  
3. BÃºsqueda semÃ¡ntica  
4. ConstrucciÃ³n del prompt RAG  
5. GeneraciÃ³n de la respuesta final  
6. Prueba con consultas reales  

---

## ğŸ§ª Ejemplo de Uso

**Entrada:**
> â€œTengo pollo y arroz, quiero algo rÃ¡pido y sin horno.â€

**Salida esperada:**
Receta recomendada: Arroz con Pollo RÃ¡pido

Corta el pollo en trozos pequeÃ±os.

Calienta una sartÃ©n y dÃ³ralo.

Agrega arroz, caldo y sal.

Cocina a fuego medio por 15 minutos.

Sirve caliente.

Resumen: receta sencilla, econÃ³mica y sin necesidad de horno.


---

## ğŸ› ï¸ InstalaciÃ³n y Uso

### Prerrequisitos
```bash
Python 3.9+
pip
torch

InstalaciÃ³n
pip install datasets sentence-transformers transformers accelerate bitsandbytes numpy torch

Clonar el repositorio
git clone https://github.com/<usuario>/<repo>.git

Ejecutar

Abrir:

notebooks/RECETAS.ipynb


Ejecutar todas las celdas.

ğŸ“¦ Modelos Utilizados
Componente	Modelo	Rol
Encoder	paraphrase-multilingual-MiniLM-L12-v2	Embeddings
Vector Store	NumPy	Almacenamiento
Decoder	TinyLlama-1.1B-Chat-v1.0	GeneraciÃ³n
Dataset	recipe_nlg_lite	Base de recetas
ğŸ“ Objetivos AcadÃ©micos

Comprender encoder vs decoder

Construir un sistema RAG real

Aplicar embeddings y similitud semÃ¡ntica

Integrar retrieval + generaciÃ³n

Implementar un asistente funcional de NLP

