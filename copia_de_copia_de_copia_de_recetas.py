# -*- coding: utf-8 -*-
"""Copia de Copia_de_Copia_de_RECETAS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GI3s7qStbGrjeFuu5kZqlSaLPZldmNZr

# Instalación de librerías

Se hace la instalación de los datasets de [Hugging Face](https://huggingface.co/datasets) y [Sentence Transformers](https://huggingface.co/sentence-transformers). También se instala una LLM de [Ollama](https://huggingface-co.translate.goog/meta-llama/Llama-2-7b?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc).
"""

# Tiempo aproximado de instalación: 2 minutos

# Librerías comunes
import time
import numpy as np
import torch
import subprocess
import json
import requests

# Instalación de datasets de Hugging Face en Colab
!pip uninstall -y datasets huggingface_hub
!pip install "datasets==3.2.0" "huggingface_hub==0.27.0"

# Instalación de los transformers sentence-transformers
!pip install -U "huggingface_hub>=0.34.0" "transformers" "sentence-transformers"

# Instalación de un modelo de Ollama
modelo = "llama2:7b"
!curl -fsSL https://ollama.com/install.sh | sh
!pkill ollama || true
!nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &
time.sleep(15)
!curl -s http://localhost:11434/api/tags || echo "El servidor Ollama no está respondiendo"
!ollama pull {modelo}

# Limpia los registros de pip y curl
time.sleep(5)
from IPython.display import clear_output
#clear_output()

"""# Importar el Dataset

Se carga el dataset de [RecipeNLG](https://huggingface.co/datasets/m3hrdadfi/recipe_nlg_lite). Este dataset contiene 7,198 recetas; por simplicidad se selecciona un subconjunto de 300 recetas.
"""

# Carga el dataset de recetas RecipeNLG
from datasets import load_dataset
print("Cargando RecipeNLG: A Cooking Recipes Dataset...")
dataset = load_dataset("m3hrdadfi/recipe_nlg_lite", trust_remote_code=True) #trust_remote_code=True se usa para evitar el prompt de "run the custom code?(y/N)"
print("\nRecipeNLG Dataset cargado:")
print(dataset)

# Toma un subconjunto de 300 recetas
#train_ds = dataset["train"].shuffle(seed=int(time.time())) #Se copia el dataset y se hace un shuffle para que la selección del subconjunto sea ALEATORIA
train_ds = dataset["train"] #Se copia el dataset (siempre va a tomar las mismas recetas la siguiente línea)
sub_ds = train_ds.select(range(300)) #Toma las primeras 300 recetas (orden aleatorio)
print("\nSe toma un subconjunto de",len(sub_ds),"recetas:")
print(sub_ds)
#print(sub_ds[0])


# Se crea una lista de recetas en formato sencillo (descarta llaves 'link' y 'ner')
recetas = []
for row in sub_ds:
    receta = {
        "id_receta": row["uid"],
        "nombre": row["name"],
        "ingredientes_txt": row["ingredients"],
        "instrucciones": row["steps"],
        "descripcion": row["description"],
    }
    recetas.append(receta)

print("\nEste es un ejemplo de receta, usando formato sencillo:")
#print(recetas[0])
recetas[0]

"""# Encoder"""

# Crea el encoder y carga el modelo

from sentence_transformers import SentenceTransformer
encoder = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# Encoding de las recetas

# Transforma las recetas (diccionarios) a texto limpio (párrafo con el nombre, ingredientes y pasos)
textos_recetas = []
for r in recetas:
    texto = f"NOMBRE: {r['nombre']}\nINGREDIENTES: {r['ingredientes_txt']}\nPASOS: {r['instrucciones']}"
    textos_recetas.append(texto)


# Codifica cada receta en un vector denso (embedding) de 384 dimensiones
embeddings_recetas = encoder.encode(
    textos_recetas,
    convert_to_numpy=True,
    show_progress_bar=True
)

print("Las",len(textos_recetas),"recetas se han transformado en vectores.")
print("La dimensión final del embedding es:", embeddings_recetas.shape)
print(embeddings_recetas.shape[0],"entradas/recetas han sido codificadas en vectores de",embeddings_recetas.shape[1]," dimensiones.")
print("\nUn ejemplo del encoding:")
print("Texto original:")
print(textos_recetas[0])
print("Embedding:")
print(embeddings_recetas[0])

"""# RAG

En esta parte se crean funciones para calcular la similaridad semántica entre un prompt (entrada) y una receta de la base de datos utilizando la distancia coseno.
"""

# Función que calcula la distancia coseno entre dos vectores
def cosine_similarity(a, b):
    a_norm = a / np.linalg.norm(a)
    b_norm = b / np.linalg.norm(b, axis=1, keepdims=True)
    return np.dot(b_norm, a_norm)

# Función que retorna los resultados con mayor simulitud semántica
def buscar_recetas(pregunta, top_k=5):
    emb_q = encoder.encode(pregunta, convert_to_numpy=True) # Hace el encoding del prompt
    sims = cosine_similarity(emb_q, embeddings_recetas) # Obtiene la distancia coseno con todas las recetas del embedding
    top_indices = np.argsort(-sims)[:top_k] # Obtiene los índices de las recetas con mayor similitud
    resultados = []
    for idx in top_indices:
        r = recetas[idx]
        score = float(sims[idx])
        resultados.append((r, score)) # Retorna las recetas (diccionario) con mayor similutud semántica y su distancia coseno
    return resultados

# Ejemplo de consulta

consulta = "Tengo pollo y arroz. Quiero algo fácil tipo sopa, rápido."
# consulta = "I have chicken and rice. I want something easy like soup, fast." # The prompt in english gives the same result as the comparisson is made based on SEMANTIC similarity
resultados = buscar_recetas(consulta, top_k=3) # Busca las 3 recetas con mayor similitud semántica


print("Consulta:",consulta)
print("Resultados:")
print("="*80)
# Muestra la distancia coseno y la receta
for r, score in resultados:
    print("SCORE:", round(score, 3))
    print("NOMBRE:", r["nombre"])
    print("INGREDIENTES:", r["ingredientes_txt"])
    print("PASOS:", r["instrucciones"][:200], "...")
    print("="*80)

"""CONTEXTO"""

consulta = "Tengo pollo y arroz. Quiero algo fácil tipo sopa, rápido."
resultados = buscar_recetas(consulta, top_k=3)

for r, score in resultados:
    print("SCORE:", round(score, 3))
    print("NOMBRE:", r["nombre"])
    print("INGREDIENTES:", r["ingredientes_txt"])
    print("PASOS:", r["instrucciones"][:200], "...")
    print("="*80)

"""# DECODING

Una vez se ha recuperado la información relevante por contenido y contexto, se hace el proceso de construcción de prompt y generación de texto final (decoding).
"""

# Función para construir el prompt a partir de los hits detectados por el RAG
def build_prompt_from_RAG(query):

    hits = buscar_recetas(query, top_k=3) # Busca recetas en la base de datos
    ctx = ""
    for i, cont in enumerate(hits):
      r, score = cont
      ctx += f"Opción de Receta #{i+1}: NOMBRE: {r["nombre"]} INGREDIENTES: {r["ingredientes_txt"]} PASOS: {r["instrucciones"]} \n"

    user = f"Pregunta del Usuario: {query}\n"
    return user + ctx

# Ejemplo
prompt_completo = build_prompt_from_RAG(consulta)
print(prompt_completo)

# Código tomado del cuaderno de transformers de la Semana 12 https://github.com/dataguirre/Curso-IA-Aplicada/blob/main/Semana%2012_Arquitectura_Transformers/transformers.ipynb

OLLAMA_URL = "http://localhost:11434"
MODEL = "llama2:7b"

# Configuración modificada
SYSTEM_PROMPT = """Eres un chatbot especializado en recomendar recetas de cocina. Espera obtener información como el nombre de un platillo o una lista de ingredientes.
Recibirás también tres o más opciones de recetas a partir de una base de datos.
Debes proporcionar información sobre el nombre de la receta, confirma los ingredientes que se necesitan, y en formato de lista dar los pasos de preparación.
Si el usuario no pregunta por una receta, no le recomiendes opciones, espera a una pregunta directa para ofrecer opciones.
Si el usuario hace preguntas sobre una receta, extiende la explicación de los pasos, pero no ofrezcas otras opciones.
Responde exclusivamente en base a las opciones de recetas obtenidas.
Responde siempre en español traduciendo las recetas cuando sea necesario.
Escribe tu respuesta ajustando mayúsculas y minúsculas en un formato claro y ordenado.
"""

# Funciones para inicializar y reiniciar contexto
def init_context(system_prompt=SYSTEM_PROMPT):
    """Crea un historial nuevo con el mensaje de sistema."""
    return [{"role": "system", "content": system_prompt.strip()}]

def reset_context(context, system_prompt=SYSTEM_PROMPT):
    """Resetea el historial manteniendo el system prompt."""
    context[:] = [{"role": "system", "content": system_prompt.strip()}]
    return context

# --- Llamada al endpoint /api/chat ---
def chat_ollama(prompt, context, model=MODEL, temperature=0.7, max_tokens=1024, seed=None, timeout=120):
    """
    Envía un turno al chat de Ollama con historial.
    - context: lista de dicts [{'role': 'system'|'user'|'assistant', 'content': str}, ...]
    Devuelve: texto de respuesta (str) y actualiza 'context' in-place.
    """
    # Añadir turno del usuario
    context.append({"role": "user", "content": str(prompt)})

    payload = {
        "model": model,
        "messages": context,
        "stream": False,  # respuesta en un único JSON
        "options": {
            "temperature": float(temperature),
            "num_predict": int(max_tokens),
        },
    }
    if seed is not None:
        payload["options"]["seed"] = int(seed)

    url = OLLAMA_URL.rstrip("/") + "/api/chat"
    resp = requests.post(url, json=payload, timeout=timeout)
    resp.raise_for_status()
    data = resp.json()

    # Extraer texto (formato /api/chat o /api/generate)
    text = ""
    if isinstance(data, dict):
        msg = data.get("message")
        if isinstance(msg, dict):
            text = msg.get("content", "")
        if not text:
            text = data.get("response", "")

    # Añadir turno del asistente
    context.append({"role": "assistant", "content": text})
    return text

# Función para preguntarle algo a la LLM
def ask(prompt, context, **kwargs):
    prompt_RAG = build_prompt_from_RAG(prompt) # Primero pasa el prompt por el RAG para obtener opciones de recetas
    text  = chat_ollama(prompt_RAG, context, **kwargs) # Le envía a Olama la pregunta inicial y las opciones
    # Imprime
    print(f'\n {prompt} \n')
    print(text)
    return text

ctx = init_context() # Inicializa contexto (SYSTEM_PROMPT)
ask("Tengo pollo y arroz. Quiero algo fácil tipo sopa, rápido.", ctx)
reset_context(ctx)  # para reiniciar la conversación

ctx = init_context() # Inicializa contexto (SYSTEM_PROMPT)
ask("Tengo pollo y arroz. Quiero algo fácil tipo sopa, rápido.", ctx)
ask("Si también tengo plátano, ¿qué puedo preparar?", ctx)
reset_context(ctx)  # para reiniciar la conversación

